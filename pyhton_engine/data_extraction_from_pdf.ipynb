{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "c01.dvi\n",
      "\n",
      "\n",
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/227988510\n",
      "\n",
      "Automatic Keyword Extraction from Individual Documents\n",
      "\n",
      "Chapter · March 2010\n",
      "\n",
      "DOI: 10.1002/9780470689646.ch1\n",
      "\n",
      "CITATIONS\n",
      "\n",
      "215\n",
      "READS\n",
      "\n",
      "35,881\n",
      "\n",
      "4 authors, including:\n",
      "\n",
      "Stuart Rose\n",
      "\n",
      "University of Texas at Austin\n",
      "\n",
      "15 PUBLICATIONS   326 CITATIONS   \n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "Dave Engel\n",
      "\n",
      "Pacific Northwest National Laboratory\n",
      "\n",
      "50 PUBLICATIONS   428 CITATIONS   \n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "Nick Cramer\n",
      "\n",
      "Pacific Northwest National Laboratory\n",
      "\n",
      "14 PUBLICATIONS   283 CITATIONS   \n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "All content following this page was uploaded by Stuart Rose on 23 October 2017.\n",
      "\n",
      "The user has requested enhancement of the downloaded file.\n",
      "\n",
      "https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_2&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_3&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_1&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/profile/Stuart_Rose?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_4&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/profile/Stuart_Rose?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_5&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/institution/University_of_Texas_at_Austin?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_6&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/profile/Stuart_Rose?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_7&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/profile/Dave_Engel?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_4&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/profile/Dave_Engel?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_5&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/institution/Pacific_Northwest_National_Laboratory?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_6&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/profile/Dave_Engel?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_7&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/profile/Nick_Cramer?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_4&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/profile/Nick_Cramer?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_5&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/institution/Pacific_Northwest_National_Laboratory?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_6&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/profile/Nick_Cramer?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_7&_esc=publicationCoverPdf\n",
      "https://www.researchgate.net/profile/Stuart_Rose?enrichId=rgreq-538ccc85c0ca9410eaa1f149ad401c9e-XXX&enrichSource=Y292ZXJQYWdlOzIyNzk4ODUxMDtBUzo1NTI2MjMwMzgwOTk0NThAMTUwODc2NzAwNzQzNA%3D%3D&el=1_x_10&_esc=publicationCoverPdf\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "Automatic keyword extraction\n",
      "from individual documents\n",
      "\n",
      "Stuart Rose, Dave Engel, Nick Cramer\n",
      "and Wendy Cowley\n",
      "\n",
      "1.1 Introduction\n",
      "\n",
      "Keywords, which we define as a sequence of one or more words, provide a\n",
      "compact representation of a document’s content. Ideally, keywords represent in\n",
      "condensed form the essential content of a document. Keywords are widely used\n",
      "to define queries within information retrieval (IR) systems as they are easy to\n",
      "define, revise, remember, and share. In comparison to mathematical signatures,\n",
      "keywords are independent of any corpus and can be applied across multiple\n",
      "corpora and IR systems.\n",
      "\n",
      "Keywords have also been applied to improve the functionality of IR sys-\n",
      "tems. Jones and Paynter (2002) describe Phrasier, a system that lists documents\n",
      "related to a primary document’s keywords, and that supports the use of keyword\n",
      "anchors as hyperlinks between documents, enabling a user to quickly access\n",
      "related material. Gutwin et al. (1999) describe Keyphind, which uses keywords\n",
      "from documents as the basic building block for an IR system. Keywords can also\n",
      "be used to enrich the presentation of search results. Hulth (2004) describes Kee-\n",
      "gle, a system that dynamically provides keyword extracts for web pages returned\n",
      "from a Google search. Andrade and Valencia (1998) present a system that auto-\n",
      "matically annotates protein function with keywords extracted from the scientific\n",
      "literature that are associated with a given protein.\n",
      "\n",
      "Text Mining: Applications and Theory edited by Michael W. Berry and Jacob Kogan\n",
      "© 2010, John Wiley & Sons, Ltd\n",
      "\n",
      "CO\n",
      "PY\n",
      "\n",
      "RI\n",
      "GH\n",
      "\n",
      "TE\n",
      "D\n",
      "\n",
      " M\n",
      "AT\n",
      "\n",
      "ER\n",
      "IA\n",
      "\n",
      "L\n",
      "\n",
      "\n",
      "\n",
      "4 TEXT MINING\n",
      "\n",
      "1.1.1 Keyword extraction methods\n",
      "\n",
      "Despite their utility for analysis, indexing, and retrieval, most documents do\n",
      "not have assigned keywords. Most existing approaches focus on the manual\n",
      "assignment of keywords by professional curators who may use a fixed taxonomy,\n",
      "or rely on the authors’ judgment to provide a representative list. Research has\n",
      "therefore focused on methods to automatically extract keywords from documents\n",
      "as an aid either to suggest keywords for a professional indexer or to generate\n",
      "summary features for documents that would otherwise be inaccessible.\n",
      "\n",
      "Early approaches to automatically extract keywords focus on evaluating\n",
      "corpus-oriented statistics of individual words. Jones (1972) and Salton et al.\n",
      "(1975) describe positive results of selecting for an index vocabulary the\n",
      "statistically discriminating words across a corpus. Later keyword extraction\n",
      "research applies these metrics to select discriminating words as keywords for\n",
      "individual documents. For example, Andrade and Valencia (1998) base their\n",
      "approach on comparison of word frequency distributions within a text against\n",
      "distributions from a reference corpus.\n",
      "\n",
      "While some keywords are likely to be evaluated as statistically discriminating\n",
      "within the corpus, keywords that occur in many documents within the corpus are\n",
      "not likely to be selected as statistically discriminating. Corpus-oriented methods\n",
      "also typically operate only on single words. This further limits the measurement of\n",
      "statistically discriminating words because single words are often used in multiple\n",
      "and different contexts.\n",
      "\n",
      "To avoid these drawbacks, we focus our interest on methods of keyword\n",
      "extraction that operate on individual documents. Such document-oriented\n",
      "methods will extract the same keywords from a document regardless of the\n",
      "current state of a corpus. Document-oriented methods therefore provide context-\n",
      "independent document features, enabling additional analytic methods such as\n",
      "those described in Engel et al. (2009) and Whitney et al. (2009) that characterize\n",
      "changes within a text stream over time. These document-oriented methods are\n",
      "suited to corpora that change, such as collections of published technical abstracts\n",
      "that grow over time or streams of news articles. Furthermore, by operating on a\n",
      "single document, these methods inherently scale to vast collections and can be\n",
      "applied in many contexts to enrich IR systems and analysis tools.\n",
      "\n",
      "Previous work on document-oriented methods of keyword extraction has com-\n",
      "bined natural language processing approaches to identify part-of-speech (POS)\n",
      "tags that are combined with supervised learning, machine-learning algorithms, or\n",
      "statistical methods.\n",
      "\n",
      "Hulth (2003) compares the effectiveness of three term selection approaches:\n",
      "noun-phrase (NP) chunks, n-grams, and POS tags, with four discriminative fea-\n",
      "tures of these terms as inputs for automatic keyword extraction using a supervised\n",
      "machine-learning algorithm.\n",
      "\n",
      "Mihalcea and Tarau (2004) describe a system that applies a series of syntactic\n",
      "filters to identify POS tags that are used to select words to evaluate as key-\n",
      "words. Co-occurrences of the selected words within a fixed-size sliding window\n",
      "\n",
      "\n",
      "\n",
      "AUTOMATIC KEYWORD EXTRACTION 5\n",
      "\n",
      "are accumulated within a word co-occurrence graph. A graph-based ranking\n",
      "algorithm (TextRank) is applied to rank words based on their associations in\n",
      "the graph, and then top ranking words are selected as keywords. Keywords that\n",
      "are adjacent in the document are combined to form multi-word keywords. Mihal-\n",
      "cea and Tarau (2004) report that TextRank achieves its best performance when\n",
      "only nouns and adjectives are selected as potential keywords.\n",
      "\n",
      "Matsuo and Ishizuka (2004) apply a chi-square measure to calculate how\n",
      "selectively words and phrases co-occur within the same sentences as a particular\n",
      "subset of frequent terms in the document text. The chi-square measure is applied\n",
      "to determine the bias of word co-occurrences in the document text which is\n",
      "then used to rank words and phrases as keywords of the document. Matsuo and\n",
      "Ishizuka (2004) state that the degree of biases is not reliable when term frequency\n",
      "is small. The authors present an evaluation on full text articles and a working\n",
      "example on a 27-page document, showing that their method operates effectively\n",
      "on large documents.\n",
      "\n",
      "In the following sections, we describe Rapid Automatic Keyword Extrac-\n",
      "tion (RAKE), an unsupervised, domain-independent, and language-independent\n",
      "method for extracting keywords from individual documents. We provide details\n",
      "of the algorithm and its configuration parameters, and present results on a bench-\n",
      "mark dataset of technical abstracts, showing that RAKE is more computationally\n",
      "efficient than TextRank while achieving higher precision and comparable recall\n",
      "scores. We then describe a novel method for generating stoplists, which we use to\n",
      "configure RAKE for specific domains and corpora. Finally, we apply RAKE to a\n",
      "corpus of news articles and define metrics for evaluating the exclusivity, essential-\n",
      "ity, and generality of extracted keywords, enabling a system to identify keywords\n",
      "that are essential or general to documents in the absence of manual annotations.\n",
      "\n",
      "1.2 Rapid automatic keyword extraction\n",
      "\n",
      "In developing RAKE, our motivation has been to develop a keyword extraction\n",
      "method that is extremely efficient, operates on individual documents to enable\n",
      "application to dynamic collections, is easily applied to new domains, and operates\n",
      "well on multiple types of documents, particularly those that do not follow specific\n",
      "grammar conventions. Figure 1.1 contains the title and text for a typical abstract,\n",
      "as well as its manually assigned keywords.\n",
      "\n",
      "RAKE is based on our observation that keywords frequently contain multiple\n",
      "words but rarely contain standard punctuation or stop words, such as the function\n",
      "words and , the, and of , or other words with minimal lexical meaning. Reviewing\n",
      "the manually assigned keywords for the abstract in Figure 1.1, there is only\n",
      "one keyword that contains a stop word (of in set of natural numbers). Stop\n",
      "words are typically dropped from indexes within IR systems and not included in\n",
      "various text analyses as they are considered to be uninformative or meaningless.\n",
      "This reasoning is based on the expectation that such words are too frequently\n",
      "and broadly used to aid users in their analyses or search tasks. Words that do\n",
      "\n",
      "\n",
      "\n",
      "6 TEXT MINING\n",
      "\n",
      "Compatibility of systems of linear constraints over the set of natural numbers\n",
      "\n",
      "Criteria of compatibility of a system of linear Diophantine equations, strict inequations, \n",
      "and nonstrict inequations are considered. Upper bounds for components of a minimal set \n",
      "of solutions and algorithms of construction of minimal generating sets of solutions for all \n",
      "types of systems are given. These criteria and the corresponding algorithms for \n",
      "constructing a minimal supporting set of solutions can be used in solving all the \n",
      "considered types of systems and systems of mixed types.\n",
      "\n",
      "Manually assigned keywords:\n",
      "linear constraints, set of natural numbers, linear Diophantine equations, strict \n",
      "inequations, nonstrict inequations, upper bounds, minimal generating sets\n",
      "\n",
      "Figure 1.1 A sample abstract from the Inspec test set and its manually assigned\n",
      "keywords.\n",
      "\n",
      "carry meaning within a document are described as content bearing and are often\n",
      "referred to as content words.\n",
      "\n",
      "The input parameters for RAKE comprise a list of stop words (or stoplist), a\n",
      "set of phrase delimiters, and a set of word delimiters. RAKE uses stop words and\n",
      "phrase delimiters to partition the document text into candidate keywords, which\n",
      "are sequences of content words as they occur in the text. Co-occurrences of words\n",
      "within these candidate keywords are meaningful and allow us to identify word co-\n",
      "occurrence without the application of an arbitrarily sized sliding window. Word\n",
      "associations are thus measured in a manner that automatically adapts to the style\n",
      "and content of the text, enabling adaptive and fine-grained measurement of word\n",
      "co-occurrences that will be used to score candidate keywords.\n",
      "\n",
      "1.2.1 Candidate keywords\n",
      "\n",
      "RAKE begins keyword extraction on a document by parsing its text into a set of\n",
      "candidate keywords. First, the document text is split into an array of words by the\n",
      "specified word delimiters. This array is then split into sequences of contiguous\n",
      "words at phrase delimiters and stop word positions. Words within a sequence are\n",
      "assigned the same position in the text and together are considered a candidate\n",
      "keyword.\n",
      "\n",
      "Figure 1.2 shows the candidate keywords in the order that they are parsed\n",
      "from the sample technical abstract shown in Figure 1.1. The candidate keyword\n",
      "\n",
      "Compatibility – systems – linear constraints – set – natural numbers – Criteria –  \n",
      "compatibility – system – linear Diophantine equations – strict inequations – nonstrict \n",
      "inequations – Upper bounds – components – minimal set – solutions – algorithms – \n",
      "minimal generating sets – solutions – systems – criteria – corresponding algorithms – \n",
      "constructing – minimal supporting set – solving – systems – systems\n",
      "\n",
      "Figure 1.2 Candidate keywords parsed from the sample abstract.\n",
      "\n",
      "\n",
      "\n",
      "AUTOMATIC KEYWORD EXTRACTION 7\n",
      "\n",
      "linear Diophantine equations begins after the stop word of and ends with a\n",
      "comma. The following word strict begins the next candidate keyword strict\n",
      "inequations .\n",
      "\n",
      "1.2.2 Keyword scores\n",
      "\n",
      "After every candidate keyword is identified and the graph of word co-occurrences\n",
      "(shown in Figure 1.3) is complete, a score is calculated for each candidate key-\n",
      "word and defined as the sum of its member word scores. We evaluated several\n",
      "metrics for calculating word scores, based on the degree and frequency of word\n",
      "vertices in the graph: (1) word frequency (freq(w)), (2) word degree (deg(w)),\n",
      "and (3) ratio of degree to frequency (deg(w)/freq(w)).\n",
      "\n",
      "The metric scores for each of the content words in the sample abstract are\n",
      "listed in Figure 1.4. In summary, deg(w) favors words that occur often and in\n",
      "longer candidate keywords; deg(minimal) scores higher than deg(systems). Words\n",
      "that occur frequently regardless of the number of words with which they co-occur\n",
      "are favored by freq(w); freq(systems) scores higher than freq(minimal). Words that\n",
      "predominantly occur in longer candidate keywords are favored by deg(w)/freq(w);\n",
      "deg(diophantine)/freq(diophantine) scores higher than deg(linear)/freq(linear).\n",
      "The score for each candidate keyword is computed as the sum of its member\n",
      "\n",
      "al\n",
      "go\n",
      "\n",
      "rit\n",
      "hm\n",
      "\n",
      "s\n",
      "\n",
      "bo\n",
      "un\n",
      "\n",
      "ds\n",
      "\n",
      "co\n",
      "m\n",
      "\n",
      "pa\n",
      "tib\n",
      "\n",
      "ili\n",
      "ty\n",
      "\n",
      "co\n",
      "m\n",
      "\n",
      "po\n",
      "ne\n",
      "\n",
      "nt\n",
      "s\n",
      "\n",
      "co\n",
      "ns\n",
      "\n",
      "tr\n",
      "ai\n",
      "\n",
      "nt\n",
      "s\n",
      "\n",
      "co\n",
      "ns\n",
      "\n",
      "tr\n",
      "uc\n",
      "\n",
      "tin\n",
      "g\n",
      "\n",
      "co\n",
      "rr\n",
      "\n",
      "es\n",
      "po\n",
      "\n",
      "nd\n",
      "in\n",
      "\n",
      "g\n",
      "\n",
      "cr\n",
      "ite\n",
      "\n",
      "ria\n",
      "\n",
      "di\n",
      "op\n",
      "\n",
      "ha\n",
      "nt\n",
      "\n",
      "in\n",
      "e\n",
      "\n",
      "eq\n",
      "ua\n",
      "\n",
      "tio\n",
      "ns\n",
      "\n",
      "ge\n",
      "ne\n",
      "\n",
      "ra\n",
      "tin\n",
      "\n",
      "g\n",
      "\n",
      "in\n",
      "eq\n",
      "\n",
      "ua\n",
      "tio\n",
      "\n",
      "ns\n",
      "\n",
      "lin\n",
      "ea\n",
      "\n",
      "r\n",
      "\n",
      "m\n",
      "in\n",
      "\n",
      "im\n",
      "al\n",
      "\n",
      "na\n",
      "tu\n",
      "\n",
      "ra\n",
      "l\n",
      "\n",
      "no\n",
      "ns\n",
      "\n",
      "tr\n",
      "ic\n",
      "\n",
      "t\n",
      "\n",
      "nu\n",
      "m\n",
      "\n",
      "be\n",
      "rs\n",
      "\n",
      "se\n",
      "t\n",
      "\n",
      "se\n",
      "ts\n",
      "\n",
      "so\n",
      "lv\n",
      "\n",
      "in\n",
      "g\n",
      "\n",
      "st\n",
      "ric\n",
      "\n",
      "t\n",
      "\n",
      "su\n",
      "pp\n",
      "\n",
      "or\n",
      "tin\n",
      "\n",
      "g\n",
      "\n",
      "sy\n",
      "st\n",
      "\n",
      "em\n",
      "\n",
      "sy\n",
      "st\n",
      "\n",
      "em\n",
      "s\n",
      "\n",
      "up\n",
      "pe\n",
      "\n",
      "r\n",
      "\n",
      "algorithms 2 1\n",
      "bounds 1 1\n",
      "compatibility 2\n",
      "components 1\n",
      "constraints 1 1\n",
      "constructing 1\n",
      "corresponding 1 1\n",
      "criteria 2\n",
      "diophantine 1 1\n",
      "equations 1\n",
      "\n",
      "1\n",
      "1 1\n",
      "\n",
      "generating 1 1 1\n",
      "\n",
      "1\n",
      "\n",
      "inequations 2 1 1\n",
      "linear 1 1 1 2\n",
      "minimal 1 3 2 1\n",
      "natural 1 1\n",
      "nonstrict 1 1\n",
      "numbers 1 1\n",
      "set 2 3 1\n",
      "sets 1 1 1\n",
      "solving 1\n",
      "strict 1 1\n",
      "supporting 1 1 1\n",
      "system 1\n",
      "systems 4\n",
      "upper 1 1\n",
      "\n",
      "Figure 1.3 The word co-occurrence graph for content words in the sample\n",
      "abstract.\n",
      "\n",
      "\n",
      "\n",
      "8 TEXT MINING\n",
      "\n",
      "al\n",
      "go\n",
      "\n",
      "rit\n",
      "hm\n",
      "\n",
      "s\n",
      "\n",
      "bo\n",
      "un\n",
      "\n",
      "ds\n",
      "\n",
      "co\n",
      "m\n",
      "\n",
      "pa\n",
      "tib\n",
      "\n",
      "ili\n",
      "ty\n",
      "\n",
      "co\n",
      "m\n",
      "\n",
      "po\n",
      "ne\n",
      "\n",
      "nt\n",
      "s\n",
      "\n",
      "co\n",
      "ns\n",
      "\n",
      "tr\n",
      "ai\n",
      "\n",
      "nt\n",
      "s\n",
      "\n",
      "co\n",
      "ns\n",
      "\n",
      "tr\n",
      "uc\n",
      "\n",
      "tin\n",
      "g\n",
      "\n",
      "co\n",
      "rr\n",
      "\n",
      "es\n",
      "po\n",
      "\n",
      "nd\n",
      "in\n",
      "\n",
      "g\n",
      "\n",
      "cr\n",
      "ite\n",
      "\n",
      "ria\n",
      "\n",
      "di\n",
      "op\n",
      "\n",
      "ha\n",
      "nt\n",
      "\n",
      "in\n",
      "e\n",
      "\n",
      "eq\n",
      "ua\n",
      "\n",
      "tio\n",
      "ns\n",
      "\n",
      "ge\n",
      "ne\n",
      "\n",
      "ra\n",
      "tin\n",
      "\n",
      "g\n",
      "\n",
      "in\n",
      "eq\n",
      "\n",
      "ua\n",
      "tio\n",
      "\n",
      "ns\n",
      "\n",
      "lin\n",
      "ea\n",
      "\n",
      "r\n",
      "\n",
      "m\n",
      "in\n",
      "\n",
      "im\n",
      "al\n",
      "\n",
      "na\n",
      "tu\n",
      "\n",
      "ra\n",
      "l\n",
      "\n",
      "no\n",
      "ns\n",
      "\n",
      "tr\n",
      "ic\n",
      "\n",
      "t\n",
      "\n",
      "nu\n",
      "m\n",
      "\n",
      "be\n",
      "rs\n",
      "\n",
      "se\n",
      "t\n",
      "\n",
      "se\n",
      "ts\n",
      "\n",
      "so\n",
      "lv\n",
      "\n",
      "in\n",
      "g\n",
      "\n",
      "st\n",
      "ric\n",
      "\n",
      "t\n",
      "\n",
      "su\n",
      "pp\n",
      "\n",
      "or\n",
      "tin\n",
      "\n",
      "g\n",
      "\n",
      "sy\n",
      "st\n",
      "\n",
      "em\n",
      "\n",
      "sy\n",
      "st\n",
      "\n",
      "em\n",
      "s\n",
      "\n",
      "up\n",
      "pe\n",
      "\n",
      "r\n",
      "\n",
      "deg(w) 3 2 2 1 2 1 2 2 3 3 3 4 5 8 2 2 2 6 3 1 2 3 1 4 2\n",
      "freq(w) 2 1 2 1 1 1 1 2 1 1 1 2 2 3 1 1 1 3 1 1 1 1 1 4 1\n",
      "deg(w) / freq(w) 1.5 2 1 1 2 1 2 1 3 3 3 2  2.5 2.7 2 2 2 2 3 1 2 3 1 1 2\n",
      "\n",
      "Figure 1.4 Word scores calculated from the word co-occurrence graph.\n",
      "\n",
      "minimal generating sets (8.7), linear diophantine equations (8.5), minimal supporting set \n",
      "(7.7), minimal set (4.7), linear constraints (4.5), natural numbers (4), strict inequations (4), \n",
      "nonstrict inequations (4), upper bounds (4), corresponding algorithms (3.5), set (2), \n",
      "algorithms (1.5), compatibility (1), systems (1), criteria (1), system (1), components \n",
      "(1),constructing (1), solving (1)\n",
      "\n",
      "Figure 1.5 Candidate keywords and their calculated scores.\n",
      "\n",
      "word scores. Figure 1.5 lists each candidate keyword from the sample abstract\n",
      "using the metric deg(w)/freq(w) to calculate individual word scores.\n",
      "\n",
      "1.2.3 Adjoining keywords\n",
      "\n",
      "Because RAKE splits candidate keywords by stop words, extracted keywords do\n",
      "not contain interior stop words. While RAKE has generated strong interest due to\n",
      "its ability to pick out highly specific terminology, an interest was also expressed\n",
      "in identifying keywords that contain interior stop words such as axis of evil . To\n",
      "find these RAKE looks for pairs of keywords that adjoin one another at least\n",
      "twice in the same document and in the same order. A new candidate keyword is\n",
      "then created as a combination of those keywords and their interior stop words.\n",
      "The score for the new keyword is the sum of its member keyword scores.\n",
      "\n",
      "It should be noted that relatively few of these linked keywords are extracted,\n",
      "which adds to their significance. Because adjoining keywords must occur twice\n",
      "in the same order within the document, their extraction is more common on texts\n",
      "that are longer than short abstracts.\n",
      "\n",
      "1.2.4 Extracted keywords\n",
      "\n",
      "After candidate keywords are scored, the top T scoring candidates are selected\n",
      "as keywords for the document. We compute T as one-third the number of words\n",
      "in the graph, as in Mihalcea and Tarau (2004).\n",
      "\n",
      "The sample abstract contains 28 content words, resulting in T = 9 key-\n",
      "words. Table 1.1 lists the keywords extracted by RAKE compared to the sample\n",
      "abstract’s manually assigned keywords. We use the statistical measures precision,\n",
      "recall and F -measure to evaluate the accuracy of RAKE. Out of nine keywords\n",
      "extracted, six are true positives; that is, they exactly match six of the manu-\n",
      "ally assigned keywords. Although natural numbers is similar to the assigned\n",
      "\n",
      "\n",
      "\n",
      "AUTOMATIC KEYWORD EXTRACTION 9\n",
      "\n",
      "Table 1.1 Comparison of keywords extracted by RAKE to\n",
      "manually assigned keywords for the sample abstract.\n",
      "\n",
      "Extracted by RAKE Manually assigned\n",
      "\n",
      "minimal generating sets minimal generating sets\n",
      "linear diophantine equations linear Diophantine equations\n",
      "minimal supporting set\n",
      "minimal set\n",
      "linear constraints linear constraints\n",
      "natural numbers\n",
      "strict inequations strict inequations\n",
      "nonstrict inequations nonstrict inequations\n",
      "upper bounds upper bounds\n",
      "\n",
      "set of natural numbers\n",
      "\n",
      "keyword set of natural numbers , for the purposes of the benchmark evaluation\n",
      "it is considered a miss. There are therefore three false positives in the set of\n",
      "extracted keywords, resulting in a precision of 67%. Comparing the six true\n",
      "positives within the set of extracted keywords to the total of seven manually\n",
      "assigned keywords results in a recall of 86%. Equally weighting precision and\n",
      "recall generates an F -measure of 75%.\n",
      "\n",
      "1.3 Benchmark evaluation\n",
      "\n",
      "To evaluate performance we tested RAKE against a collection of technical\n",
      "abstracts used in the keyword extraction experiments reported in Hulth (2003)\n",
      "and Mihalcea and Tarau (2004), mainly for the purpose of allowing direct\n",
      "comparison with their results.\n",
      "\n",
      "1.3.1 Evaluating precision and recall\n",
      "\n",
      "The collection consists of 2000 Inspec abstracts for journal papers from Computer\n",
      "Science and Information Technology. The abstracts are divided into a training\n",
      "set with 1000 abstracts, a validation set with 500 abstracts, and a testing set with\n",
      "500 abstracts. We followed the approach described in Mihalcea and Tarau (2004),\n",
      "using the testing set for evaluation because RAKE does not require a training\n",
      "set. Extracted keywords for each abstract are compared against the abstract’s\n",
      "associated set of manually assigned uncontrolled keywords.\n",
      "\n",
      "Table 1.2 details RAKE’s performance using a generated stoplist, Fox’s sto-\n",
      "plist (Fox 1989), and T as one-third the number of words in the graph. For\n",
      "each method, which corresponds to a row in the table, the following information\n",
      "is shown: the total number of extracted keywords and mean per abstract; the\n",
      "number of correct extracted keywords and mean per abstract; precision; recall;\n",
      "and F -measure. Results published within Hulth (2003) and Mihalcea and Tarau\n",
      "\n",
      "\n",
      "\n",
      "10 TEXT MINING\n",
      "\n",
      "Table 1.2 Results of automatic keyword extraction on 500 abstracts in the\n",
      "Inspec test set using RAKE, TextRank (Mihalcea and Tarau 2004) and\n",
      "supervised learning (Hulth 2003).\n",
      "\n",
      "Extracted Correct\n",
      "keywords keywords\n",
      "\n",
      "Method Total Mean Total Mean Precision Recall F -measure\n",
      "\n",
      "RAKE (T = 0.33)\n",
      "KA stoplist (df > 10) 6052 12.1 2037 4.1 33.7 41.5 37.2\n",
      "Fox stoplist 7893 15.8 2054 4.2 26 42.2 32.1\n",
      "\n",
      "TextRank\n",
      "Undirected, co-occ.\n",
      "\n",
      "window = 2\n",
      "6784 13.6 2116 4.2 31.2 43.1 36.2\n",
      "\n",
      "Undirected, co-occ.\n",
      "window = 3\n",
      "\n",
      "6715 13.4 1897 3.8 28.2 38.6 32.6\n",
      "\n",
      "(Hulth 2003)\n",
      "Ngram with tag 7815 15.6 1973 3.9 25.2 51.7 33.9\n",
      "NP chunks with tag 4788 9.6 1421 2.8 29.7 37.2 33\n",
      "Pattern with tag 7012 14 1523 3 21.7 39.9 28.1\n",
      "\n",
      "the, and, of, a, in, is, for, to, we, this, are, with, as, on, it, an, that, which, by, using, can, \n",
      "paper, from, be, based, has, was, have, or, at, such, also, but, results, proposed, show, \n",
      "new, these, used, however, our, were, when, one, not, two, study, present, its, sub, both, \n",
      "then, been, they, all, presented, if, each, approach, where, may, some, more, use, \n",
      "between, into, 1, under, while, over, many, through, addition, well, first, will, there, \n",
      "propose, than, their, 2, most, sup, developed, particular, provides, including, other, how, \n",
      "without, during, article, application, only, called, what, since, order, experimental, any\n",
      "\n",
      "Figure 1.6 Top 100 words in the generated stoplist.\n",
      "\n",
      "(2004) are included for comparison. The highest values for precision, recall, and\n",
      "F -measure are shown in bold. As noted, perfect precision is not possible with\n",
      "any of the techniques as the manually assigned keywords do not always appear\n",
      "in the abstract text. The highest precision and F -measure are achieved using\n",
      "RAKE with a generated stoplist based on keyword adjacency, a subset of which\n",
      "is listed in Figure 1.6. With this stoplist RAKE yields the best results in terms of\n",
      "F -measure and precision, and provides comparable recall. With Fox’s stoplist,\n",
      "RAKE achieves a high recall while experiencing a drop in precision.\n",
      "\n",
      "1.3.2 Evaluating efficiency\n",
      "\n",
      "Because of increasing interest in energy conservation in large data centers, we\n",
      "also evaluated the computational cost associated with extracting keywords with\n",
      "RAKE and TextRank. TextRank applies syntactic filters to a document text to\n",
      "\n",
      "\n",
      "\n",
      "AUTOMATIC KEYWORD EXTRACTION 11\n",
      "\n",
      "identify content words and accumulates a graph of word co-occurrences in a\n",
      "window size of 2. A rank for each word in the graph is calculated through a\n",
      "series of iterations until convergence below a threshold is achieved.\n",
      "\n",
      "We set TextRank’s damping factor d = 0.85 and its convergence threshold to\n",
      "0.0001, as recommended in Mihalcea and Tarau (2004). We do not have access\n",
      "to the syntactic filters referenced in Mihalcea and Tarau (2004), so were unable\n",
      "to evaluate their computational cost.\n",
      "\n",
      "To minimize disparity, all parsing stages in the respective extraction methods\n",
      "are identical, TextRank accumulates co-occurrences in a window of size 2, and\n",
      "RAKE accumulates word co-occurrences within candidate keywords. After co-\n",
      "occurrences are tallied, the algorithms compute keyword scores according to their\n",
      "respective methods. The benchmark was implemented in Java and executed in the\n",
      "Java SE Runtime Environment (JRE) 6 on a Dell Precision T7400 workstation.\n",
      "\n",
      "We calculated the total time for RAKE and TextRank (as an average over 100\n",
      "iterations) to extract keywords from the Inspec testing set of 500 abstracts, after\n",
      "the abstracts were read from files and loaded in memory. RAKE extracted key-\n",
      "words from the 500 abstracts in 160 milliseconds. TextRank extracted keywords\n",
      "in 1002 milliseconds, over 6 times the time of RAKE.\n",
      "\n",
      "Referring to Figure 1.7, we can see that as the number of content words\n",
      "for a document increases, the performance advantage of RAKE over TextRank\n",
      "increases. This is due to RAKE’s ability to score keywords in a single pass\n",
      "whereas TextRank requires repeated iterations to achieve convergence on\n",
      "word ranks.\n",
      "\n",
      "Based on this benchmark evaluation, it is clear that RAKE effectively extracts\n",
      "keywords and outperforms the current state of the art in terms of precision, effi-\n",
      "ciency, and simplicity. As RAKE can be put to use in many different systems and\n",
      "applications, in the next section we discuss a method for stoplist generation that\n",
      "may be used to configure RAKE on particular corpora, domains, and languages.\n",
      "\n",
      "1.4 Stoplist generation\n",
      "\n",
      "Stoplists are widely used in IR and text analysis applications. However, there is\n",
      "remarkably little information describing methods for their creation. Fox (1989)\n",
      "presents an analysis of stoplists, noting discrepancies between stated conven-\n",
      "tions and actual instances and implementations of stoplists. The lack of tech-\n",
      "nical rigor associated with the creation of stoplists presents a challenge when\n",
      "comparing text analysis methods. In practice, stoplists are often based on com-\n",
      "mon function words and hand-tuned for particular applications, domains, or\n",
      "specific languages.\n",
      "\n",
      "We evaluated the use of term frequency as a metric for automatically selecting\n",
      "words for a stoplist. Table 1.3 lists the top 50 words by term frequency in the\n",
      "training set of abstracts in the benchmark dataset. Additional metrics shown for\n",
      "each word are document frequency, adjacency frequency, and keyword frequency.\n",
      "Adjacency frequency reflects the number of times the word occurred adjacent to\n",
      "\n",
      "\n",
      "\n",
      "12 TEXT MINING\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "7\n",
      "\n",
      "10 20 30 40 50 60 70 80 90 100 110 120\n",
      "\n",
      "M\n",
      "ill\n",
      "\n",
      "is\n",
      "ec\n",
      "\n",
      "on\n",
      "ds\n",
      "\n",
      "Number of Vertices in Word Co -occurrence Graph\n",
      "\n",
      "Extraction Time by Document Size\n",
      "\n",
      "RAKE\n",
      "\n",
      "TextRank\n",
      "\n",
      "Figure 1.7 Comparison of TextRank and RAKE extraction times on individual\n",
      "documents.\n",
      "\n",
      "an abstract’s keywords. Keyword frequency reflects the number of times the word\n",
      "occurred within an abstract’s keywords.\n",
      "\n",
      "Looking at the top 50 frequent words, in addition to the typical function\n",
      "words, we can see that system, control , and method are highly frequent within\n",
      "technical abstracts and highly frequent within the abstracts’ keywords. Selecting\n",
      "solely by term frequency will therefore cause content-bearing words to be added\n",
      "to the stoplist, particularly if the corpus of documents is focused on a particular\n",
      "domain or topic. In those circumstances, selecting stop words by term frequency\n",
      "presents a risk of removing important content-bearing words from analysis.\n",
      "\n",
      "We therefore present the following method for automatically generating a\n",
      "stoplist from a set of documents for which keywords are defined. The algorithm\n",
      "is based on the intuition that words adjacent to, and not within, keywords are\n",
      "less likely to be meaningful and therefore are good choices for stop words.\n",
      "\n",
      "To generate our stoplist we identified for each abstract in the Inspec training\n",
      "set the words occurring adjacent to words in the abstract’s uncontrolled key-\n",
      "word list. The frequency of each word occurring adjacent to a keyword was\n",
      "accumulated across the abstracts. Words that occurred more frequently within\n",
      "keywords than adjacent to them were excluded from the stoplist.\n",
      "\n",
      "\n",
      "\n",
      "AUTOMATIC KEYWORD EXTRACTION 13\n",
      "\n",
      "Table 1.3 The 50 most frequent words in the Inspec training set listed in\n",
      "descending order by term frequency.\n",
      "\n",
      "Term Document Adjacency Keyword\n",
      "Word frequency frequency frequency frequency\n",
      "\n",
      "the 8611 978 3492 3\n",
      "of 5546 939 1546 68\n",
      "and 3644 911 2104 23\n",
      "a 3599 893 1451 2\n",
      "to 3000 879 792 10\n",
      "in 2656 837 1402 7\n",
      "is 1974 757 1175 0\n",
      "for 1912 767 951 9\n",
      "that 1129 590 330 0\n",
      "with 1065 577 535 3\n",
      "are 1049 576 555 1\n",
      "this 964 581 645 0\n",
      "on 919 550 340 8\n",
      "an 856 501 332 0\n",
      "we 822 388 731 0\n",
      "by 773 475 283 0\n",
      "as 743 435 344 0\n",
      "be 595 395 170 0\n",
      "it 560 369 339 13\n",
      "system 507 255 86 202\n",
      "can 452 319 250 0\n",
      "based 451 293 168 15\n",
      "from 447 309 187 0\n",
      "using 428 282 260 0\n",
      "control 409 166 12 237\n",
      "which 402 280 285 0\n",
      "paper 398 339 196 1\n",
      "systems 384 194 44 191\n",
      "method 347 188 78 85\n",
      "data 347 159 39 131\n",
      "time 345 201 24 95\n",
      "model 343 157 37 122\n",
      "information 322 153 18 151\n",
      "or 315 218 146 0\n",
      "s 314 196 27 0\n",
      "have 301 219 149 0\n",
      "has 297 225 166 0\n",
      "at 296 216 141 0\n",
      "new 294 197 93 4\n",
      "two 287 205 83 5\n",
      "\n",
      "(continued overleaf )\n",
      "\n",
      "\n",
      "\n",
      "14 TEXT MINING\n",
      "\n",
      "Table 1.3 (Continued )\n",
      "\n",
      "Term Document Adjacency Keyword\n",
      "Word frequency frequency frequency frequency\n",
      "\n",
      "algorithm 267 123 36 96\n",
      "results 262 221 129 14\n",
      "used 262 204 92 0\n",
      "was 254 125 161 0\n",
      "these 252 200 93 0\n",
      "also 251 219 139 0\n",
      "such 249 198 140 0\n",
      "problem 234 137 36 55\n",
      "design 225 110 38 68\n",
      "\n",
      "To evaluate this method of generating stoplists, we created six stoplists, three\n",
      "of which select words for the stoplist by term frequency (TF), and three which\n",
      "select words by term frequency but also exclude words from the stoplist whose\n",
      "keyword frequency was greater than their keyword adjacency frequency. We\n",
      "refer to this latter set of stoplists as keyword adjacency (KA) stoplists since they\n",
      "primarily include words that are adjacent to and not within keywords.\n",
      "\n",
      "Table 1.4 Comparison of RAKE performance using stoplists based on term\n",
      "frequency (TF) and keyword adjacency (KA).\n",
      "\n",
      "Extracted Correct\n",
      "keywords keywords\n",
      "\n",
      "Stoplist\n",
      "Method size Total Mean Total Mean Precision Recall F -measure\n",
      "\n",
      "RAKE\n",
      "(T = 0.33)\n",
      "\n",
      "TF stoplist\n",
      "(df > 10)\n",
      "\n",
      "1347 3670 7.3 606 1.2 16.5 12.3 14.1\n",
      "\n",
      "TF stoplist\n",
      "(df > 25)\n",
      "\n",
      "527 5563 11.1 1032 2.1 18.6 21.0 19.7\n",
      "\n",
      "TF stoplist\n",
      "(df > 50)\n",
      "\n",
      "205 7249 14.5 1520 3.0 21.0 30.9 25.0\n",
      "\n",
      "RAKE\n",
      "(T = 0.33)\n",
      "\n",
      "KA stoplist\n",
      "(df > 10)\n",
      "\n",
      "763 6052 12.1 2037 4.1 33.7 41.5 37.2\n",
      "\n",
      "KA stoplist\n",
      "(df > 25)\n",
      "\n",
      "325 7079 14.2 2103 4.3 29.7 42.8 35.1\n",
      "\n",
      "KA stoplist\n",
      "(df > 50)\n",
      "\n",
      "147 8013 16.0 2117 4.3 26.4 43.1 32.8\n",
      "\n",
      "\n",
      "\n",
      "AUTOMATIC KEYWORD EXTRACTION 15\n",
      "\n",
      "Each of the stoplists was set as the input stoplist for RAKE, which was\n",
      "then run on the testing set of the Inspec corpus of technical abstracts. Table 1.4\n",
      "lists the precision, recall, and F -measure for the keywords extracted by each\n",
      "of these runs. The KA stoplists generated by our method outperformed the\n",
      "TF stoplists generated by term frequency. A notable difference between results\n",
      "achieved using the two types of stoplists is evident in Table 1.4: the F -measure\n",
      "improves as more words are added to a KA stoplist, whereas when more words are\n",
      "added to a TF stoplist the F -measure degrades. Furthermore, the best TF stoplist\n",
      "underperforms the worst KA stoplist. This verifies that our algorithm for gener-\n",
      "ating stoplists is adding the right stop words and excluding content words from\n",
      "the stoplist.\n",
      "\n",
      "Because the generated KA stoplists leverage manually assigned keywords, we\n",
      "envision that an ideal application would be within existing digital libraries or IR\n",
      "systems and collections where defined keywords exist or are easily identified for\n",
      "a subset of the documents. Stoplists only need to be generated once for particular\n",
      "domains, enabling RAKE to be applied to new and future articles, facilitating\n",
      "the annotation and indexing of new documents.\n",
      "\n",
      "1.5 Evaluation on news articles\n",
      "\n",
      "While we have shown that a simple set of configuration parameters enables\n",
      "RAKE to efficiently extract keywords from individual documents, it is worth\n",
      "investigating how well extracted keywords represent the essential content within\n",
      "a corpus of documents for which keywords have not been manually assigned.\n",
      "The following section presents results on application of RAKE to the Multi-\n",
      "Perspective Question Answering (MPQA) Corpus (CERATOPS 2009).\n",
      "\n",
      "1.5.1 The MPQA Corpus\n",
      "\n",
      "The MPQA Corpus consists of 535 news articles provided by the Center for the\n",
      "Extraction and Summarization of Events and Opinions in Text (CERATOPS).\n",
      "Articles in the MPQA Corpus are from 187 different foreign and US news sources\n",
      "and date from June 2001 to May 2002.\n",
      "\n",
      "1.5.2 Extracting keywords from news articles\n",
      "\n",
      "We extracted keywords from title and text fields of documents in the MPQA\n",
      "Corpus and set a minimum document threshold of two because we are interested\n",
      "in keywords that are associated with multiple documents.\n",
      "\n",
      "Candidate keyword scores were based on word scores as deg(w)/freq(w)\n",
      "and as deg(w). Calculating word scores as deg(w)/freq(w), RAKE extracted 517\n",
      "keywords referenced by an average of 4.9 documents. Calculating word scores\n",
      "as deg(w), RAKE extracted 711 keywords referenced by an average of 8.1\n",
      "documents.\n",
      "\n",
      "\n",
      "\n",
      "16 TEXT MINING\n",
      "\n",
      "This difference in average number of referenced document counts is the\n",
      "result of longer keywords having lower frequency across documents. The metric\n",
      "deg(w)/freq(w) favors longer keywords and therefore results in extracted key-\n",
      "words that occur in fewer documents in the MPQA Corpus.\n",
      "\n",
      "In many cases a subject is occasionally presented in its long form and more\n",
      "frequently referenced in its shorter form. For example, referring to Table 1.5,\n",
      "kyoto protocol on climate change and 1997 kyoto protocol occur less frequently\n",
      "than the shorter kyoto protocol . Because our interest in the analysis of news\n",
      "articles is to connect articles that reference related content, we set RAKE to\n",
      "score words by deg(w) in order to favor shorter keywords that occur across more\n",
      "documents.\n",
      "\n",
      "Because most documents are unique within any given corpus, we expect to\n",
      "find variability in what documents are essentially about as well as how each\n",
      "document represents specific subjects. While some documents may be primarily\n",
      "about the kyoto protocol , greenhouse gas emissions , and climate change, other\n",
      "documents may only make references to those subjects. Documents in the former\n",
      "set will likely have kyoto protocol , greenhouse gas emissions , and climate change\n",
      "extracted as keywords whereas documents in the latter set will not.\n",
      "\n",
      "In many applications, users have a desire to capture all references to extracted\n",
      "keywords. For the purposes of evaluating extracted keywords, we accumulate\n",
      "\n",
      "Table 1.5 Keywords extracted with word scores by deg(w) and deg(w)/freq(w).\n",
      "\n",
      "Scored by deg(w) Scored by deg(w)/\n",
      "freq(w)\n",
      "\n",
      "Keyword edf(w) rdf(w) edf(w) rdf(w)\n",
      "\n",
      "kyoto protocol legally obliged\n",
      "developed countries\n",
      "\n",
      "2 2 2 2\n",
      "\n",
      "eu leader urge russia to ratify\n",
      "kyoto protocol\n",
      "\n",
      "2 2 2 2\n",
      "\n",
      "kyoto protocol on climate\n",
      "change\n",
      "\n",
      "2 2 2 2\n",
      "\n",
      "ratify kyoto protocol 2 2 2 2\n",
      "kyoto protocol requires 2 2 2 2\n",
      "1997 kyoto protocol 2 4 4 4\n",
      "kyoto protocol 31 44 7 44\n",
      "kyoto 10 12 – –\n",
      "kyoto accord 3 3 – –\n",
      "kyoto pact 2 3 – –\n",
      "sign kyoto protocol 2 2 – –\n",
      "ratification of the kyoto\n",
      "\n",
      "protocol\n",
      "2 2 – –\n",
      "\n",
      "ratify the kyoto protocol 2 2 – –\n",
      "kyoto agreement 2 2 – –\n",
      "\n",
      "\n",
      "\n",
      "AUTOMATIC KEYWORD EXTRACTION 17\n",
      "\n",
      "counts on how often each extracted keyword is referenced by documents in the\n",
      "corpus. The referenced document frequency of a keyword, rdf(k), is the number of\n",
      "documents in which the keyword occurred as a candidate keyword. The extracted\n",
      "document frequency of a keyword, edf(k), is the number of documents from which\n",
      "the keyword was extracted.\n",
      "\n",
      "A keyword that is extracted from all of the documents in which it is refer-\n",
      "enced can be characterized as exclusive or essential , whereas a keyword that is\n",
      "referenced in many documents but extracted from a few may be characterized as\n",
      "general . Comparing the relationship of edf(k) and rdf(k) allows us to characterize\n",
      "the exclusivity of a particular keyword. We therefore define keyword exclusivity\n",
      "exc(k) as shown in Equation (1.1):\n",
      "\n",
      "exc(k) = edf(k)\n",
      "rdf(k)\n",
      "\n",
      ". (1.1)\n",
      "\n",
      "Of the 711 extracted keywords, 395 have an exclusivity score of 1, indicating\n",
      "that they were extracted from every document in which they were referenced.\n",
      "Within that set of 395 exclusive keywords, some occur in more documents than\n",
      "others and can therefore be considered more essential to the corpus of documents.\n",
      "In order to measure how essential a keyword is, we define the essentiality of a\n",
      "keyword, ess(k), as shown in Equation (1.2):\n",
      "\n",
      "ess(k) = exc(k) × edf(k). (1.2)\n",
      "Figure 1.8 lists the top 50 essential keywords extracted from the MPQA cor-\n",
      "\n",
      "pus, listed in descending order by their ess(k) scores. According to CERATOPS,\n",
      "the MPQA corpus comprises 10 primary topics, listed in Table 1.6, which are\n",
      "well represented by the 50 most essential keywords as extracted and ranked by\n",
      "RAKE.\n",
      "\n",
      "In addition to keywords that are essential to documents, we can also char-\n",
      "acterize keywords by how general they are to the corpus. In other words, how\n",
      "\n",
      "united states (32), human rights (24), kyoto protocol (22), international space station (18), \n",
      "mugabe (16), space station (14), human rights report (12), greenhouse gas emissions \n",
      "(12), chavez (11), taiwan issue (11), president chavez (10), human rights violations (10), \n",
      "president bush (10), palestinian people (10), prisoners of war (9), president hugo chavez \n",
      "(9), kyoto (8), taiwan (8), israeli government (8), hugo chavez (8), climate change (8), \n",
      "space (8), axis of evil (7), president fernando henrique cardoso (7), palestinian (7), \n",
      "palestinian territories (6), taiwan strait (6), russian news agency interfax (6), prisoners (6), \n",
      "taiwan relations act (6), president robert mugabe (6), presidential election (6), geneva \n",
      "convention (5), palestinian authority (5), venezuelan president hugo chavez (5), chinese \n",
      "president jiang zemin (5), opposition leader morgan tsvangirai (5), french news agency \n",
      "afp (5), bush (5), north korea (5), camp x-ray (5), rights (5), election (5), mainland china \n",
      "(5), al qaeda (5), president (4), south africa (4), global warming (4), bush administration \n",
      "(4), mdc leader (4)\n",
      "\n",
      "Figure 1.8 Top 50 essential keywords from the MPQA Corpus, with correspond-\n",
      "ing ess(k) score in parentheses.\n",
      "\n",
      "\n",
      "\n",
      "18 TEXT MINING\n",
      "\n",
      "Table 1.6 MPQA Corpus topics and definitions.\n",
      "\n",
      "Topic Description\n",
      "\n",
      "argentina Economic collapse in Argentina\n",
      "axisofevil Reaction to President Bush’s 2002 State of the Union Address\n",
      "guantanamo US holding prisoners in Guantanamo Bay\n",
      "humanrights Reaction to US State Department report on human rights\n",
      "kyoto Ratification of Kyoto Protocol\n",
      "mugabe 2002 Presidential election in Zimbabwe\n",
      "settlements Israeli settlements in Gaza and West Bank\n",
      "spacestation Space missions of various countries\n",
      "taiwan Relations between Taiwan and China\n",
      "venezuela Presidential coup in Venezuela\n",
      "\n",
      "government (147), countries (141), people (125), world (105), report (91), war (85), united \n",
      "states (79), china (71), president (69), iran (60), bush (56), japan (50), law (44), peace \n",
      "(44), policy (43), officials (43), israel (41), zimbabwe (39), taliban (36), prisoners (35), \n",
      "opposition (35), plan (35), president george (34), axis (34), administration (33), detainees \n",
      "(32), treatment (32), states (30), european union (30), palestinians (30), election (29), \n",
      "rights (28), international community (27), military (27), argentina (27), america (27), \n",
      "guantanamo bay (26), official (26), weapons (24), source (24), eu (23), attacks (23), \n",
      "united nations (22), middle east (22), bush administration (22), human rights (21), base \n",
      "(20), minister (20), party (19), north korea (18) \n",
      "\n",
      "Figure 1.9 Top 50 general keywords from the MPQA Corpus, with corresponding\n",
      "gen(k) score in parentheses.\n",
      "\n",
      "often was a keyword referenced by documents from which it was not extracted?\n",
      "In this case we define generality of a keyword, gen(k), as shown in Equation\n",
      "(1.3):\n",
      "\n",
      "gen(k) = rdf(k) × (1.0 − exc(k)). (1.3)\n",
      "Figure 1.9 lists the top 50 general keywords extracted from the MPQA corpus,\n",
      "\n",
      "listed in descending order by their gen(k) scores. It should be noted that general\n",
      "keywords and essential keywords are not mutually exclusive. Within the top 50\n",
      "for both metrics, there are several shared keywords: united states , president ,\n",
      "bush , prisoners , election , rights , bush administration , human rights , and north\n",
      "korea . Keywords that are both highly essential and highly general are essential\n",
      "to a set of documents within the corpus but also referenced by a significantly\n",
      "greater number of documents within the corpus than other keywords.\n",
      "\n",
      "1.6 Summary\n",
      "\n",
      "We have shown that our automatic keyword extraction technology, RAKE,\n",
      "achieves higher precision and similar recall in comparison to existing techniques.\n",
      "\n",
      "\n",
      "\n",
      "AUTOMATIC KEYWORD EXTRACTION 19\n",
      "\n",
      "In contrast to methods that depend on natural language processing techniques\n",
      "to achieve their results, RAKE takes a simple set of input parameters and\n",
      "automatically extracts keywords in a single pass, making it suitable for a wide\n",
      "range of documents and collections.\n",
      "\n",
      "Finally, RAKE’s simplicity and efficiency enable its use in many applications\n",
      "where keywords can be leveraged. Based on the variety and volume of existing\n",
      "collections and the rate at which documents are created and collected, RAKE\n",
      "provides advantages and frees computing resources for other analytic methods.\n",
      "\n",
      "1.7 Acknowledgements\n",
      "\n",
      "This work was supported by the National Visualization and Analytics Center™\n",
      "(NVAC™), which is sponsored by the US Department of Homeland Security\n",
      "Program and located at the Pacific Northwest National Laboratory (PNNL), and\n",
      "by Laboratory Directed Research and Development at PNNL. PNNL is managed\n",
      "for the US Department of Energy by Battelle Memorial Institute under Contract\n",
      "DE-AC05-76RL01830.\n",
      "\n",
      "We also thank Anette Hulth, for making available the dataset used in her\n",
      "experiments.\n",
      "\n",
      "References\n",
      "\n",
      "Andrade M and Valencia A 1998 Automatic extraction of keywords from scientific\n",
      "text: application to the knowledge domain of protein families. Bioinformatics 14(7),\n",
      "600–607.\n",
      "\n",
      "CERATOPS 2009 MPQA Corpus http://www.cs.pitt.edu/mpqa/ceratops/corpora.html.\n",
      "Engel D, Whitney P, Calapristi A and Brockman F 2009 Mining for emerging technolo-\n",
      "\n",
      "gies within text streams and documents. Proceedings of the Ninth SIAM International\n",
      "Conference on Data Mining . Society for Industrial and Applied Mathematics.\n",
      "\n",
      "Fox C 1989 A stop list for general text. ACM SIGIR Forum , vol. 24, pp. 19–21. ACM,\n",
      "New York, USA.\n",
      "\n",
      "Gutwin C, Paynter G, Witten I, Nevill-Manning C and Frank E 1999 Improving browsing\n",
      "in digital libraries with keyphrase indexes. Decision Support Systems 27(1–2), 81–104.\n",
      "\n",
      "Hulth A 2003 Improved automatic keyword extraction given more linguistic knowledge.\n",
      "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Pro-\n",
      "cessing , vol. 10, pp. 216–223 Association for Computational Linguistics, Morristown,\n",
      "NJ, USA.\n",
      "\n",
      "Hulth A 2004 Combining machine learning and natural language processing for automatic\n",
      "keyword extraction . Stockholm University, Faculty of Social Sciences, Department of\n",
      "Computer and Systems Sciences (together with KTH).\n",
      "\n",
      "Jones K 1972 A statistical interpretation of term specificity and its application in retrieval.\n",
      "Journal of Documentation 28(1), 11–21.\n",
      "\n",
      "Jones S and Paynter G 2002 Automatic extraction of document keyphrases for use in\n",
      "digital libraries: evaluation and applications. Journal of the American Society for Infor-\n",
      "mation Science and Technology .\n",
      "\n",
      "\n",
      "\n",
      "20 TEXT MINING\n",
      "\n",
      "Matsuo Y and Ishizuka M 2004 Keyword extraction from a single document using word\n",
      "co-occurrence statistical information. International Journal on Artificial Intelligence\n",
      "Tools 13(1), 157–169.\n",
      "\n",
      "Mihalcea R and Tarau P 2004 Textrank: Bringing order into texts. In Proceedings of\n",
      "EMNLP 2004 (ed. Lin D and Wu D), pp. 404–411. Association for Computational\n",
      "Linguistics, Barcelona, Spain.\n",
      "\n",
      "Salton G, Wong A and Yang C 1975 A vector space model for automatic indexing.\n",
      "Communications of the ACM 18(11), 613–620.\n",
      "\n",
      "Whitney P, Engel D and Cramer N 2009 Mining for surprise events within text streams.\n",
      "Proceedings of the Ninth SIAM International Conference on Data Mining , pp. 617–627.\n",
      "Society for Industrial and Applied Mathematics.\n",
      "\n",
      "View publication statsView publication stats\n",
      "\n",
      "https://www.researchgate.net/publication/227988510\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tika import parser\n",
    "\n",
    "raw = parser.from_file('/home/kira/Documents/maroon intern/Automatic_Keyword_Extraction_from_Individual_Docum.pdf')\n",
    "print(raw['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['word','line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "needle=\"Phrasier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in raw['content'].splitlines():\n",
    "    if needle in i:\n",
    "        df=df.append({'word':needle,'line':i},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phrasier</td>\n",
       "      <td>tems. Jones and Paynter (2002) describe Phrasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word                                               line\n",
       "0  Phrasier  tems. Jones and Paynter (2002) describe Phrasi..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tems. Jones and Paynter (2002) describe Phrasier, a system that lists documents'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.line[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"word\":\"Phrasier\",\"line\":\"tems. Jones and Paynter (2002) describe Phrasier, a system that lists documents\"}]'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
